# -*- coding: utf-8 -*-
"""Preprocessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MZ_OffeRByJZERY1pLEGX8yZ4jzBSK1J

# **Preprocessing**

### **Mount Google Drive**
This code connects your Google Drive to Colab so you can access your files.
"""

from google.colab import drive
drive.mount('/content/drive')

"""### **Importing Required Libraries**
This section imports the essential libraries needed for the notebook:

1. **PyTorch (`torch`)**: A framework for deep learning and tensor operations.
2. **NiBabel (`nib`)**: Used to handle NIfTI medical images.
3. **Matplotlib (`plt`)**: For visualizing images and graphs.
"""

!pip install nibabel
import torch
import nibabel as nib
import matplotlib.pyplot as plt

"""###  **Process and Visualize NIfTI Data**

- **Read NIfTI Files**: Load medical images from `.nii` files and convert them into PyTorch tensors.  
- **Filter Liver Region**: Apply intensity thresholds to focus on the liver region (e.g., 0-200).  
- **Slice 3D Tensor**: Break down 3D medical images into individual 2D slices for processing.  
- **Display Slices**: Visualize specific slices with metadata, showing both the volume slice and the segmentation mask side by side.  

"""

# Read NIfTI files
def read_nii_file(file_path, device="cpu"):
    nii_data = nib.load(file_path)
    image_array = nii_data.get_fdata()
    return torch.from_numpy(image_array).float().to(device)

# Filter liver region based on intensity
def filter_liver_region(tensor, lower=0, upper=200):
    mask = (tensor >= lower) & (tensor <= upper)
    filtered_tensor = tensor * mask
    return filtered_tensor

# Slice 3D tensor into 2D slices
def slice_nii_tensor(tensor):
    num_slices = tensor.shape[2]
    return [tensor[:, :, i] for i in range(num_slices)]
import matplotlib.pyplot as plt

# Function to visualize a slice from the dataset
def display_slice(dataset, idx):
    # Retrieve the volume slice, segmentation slice, and metadata
    volume_slice, segment_slice, metadata = dataset[idx]

    # Convert tensors to CPU and detach for visualization
    volume_slice = volume_slice.squeeze().cpu().numpy()  # Squeeze to remove channel dimension
    segment_slice = segment_slice.squeeze().cpu().numpy()

    # Display the volume and segmentation side by side
    plt.figure(figsize=(10, 5))

    # Display the volume slice
    plt.subplot(1, 2, 1)
    plt.title(f"Volume Slice {idx}")
    plt.imshow(volume_slice, cmap="gray")
    plt.axis("off")

    # Display the segmentation slice
    plt.subplot(1, 2, 2)
    plt.title(f"Segmentation Slice {idx}")
    plt.imshow(segment_slice, cmap="gray")
    plt.axis("off")

    plt.show()

"""### **Dataset: `LiverTumorDataset`**

- Converts 3D `.nii` files to 2D slices.  
- Filters liver region (intensity 0-230).  
- Stores slices with metadata for easy access.  
- Methods:  
  - `__len__`: Total slices.  
  - `__getitem__`: Get slice, segmentation, metadata.  

"""

import torch

class LiverTumorDataset(torch.utils.data.Dataset):
    def __init__(self, volume_paths, segmentation_paths, device="cpu"):
        self.volume_paths = volume_paths
        self.segmentation_paths = segmentation_paths
        self.device = device

        # Initialize lists to store slices and metadata
        self.all_volume_slices = []
        self.all_segment_slices = []
        self.volume_metadata = []  # Metadata for each volume

        # Prepare slices and metadata
        self._prepare_slices()

    def _prepare_slices(self):
        for volume_idx, (volume_path, segmentation_path) in enumerate(zip(self.volume_paths, self.segmentation_paths)):
            # Load volume and segmentation tensors
            volume_tensor = read_nii_file(volume_path, device=self.device).float()  # Ensure tensor is float
            segment_tensor = read_nii_file(segmentation_path, device=self.device).float()  # Ensure tensor is float

            # Filter liver region and slice both volume and segmentation
            filtered_volume_tensor = filter_liver_region(volume_tensor, lower=0, upper=230)
            volume_slices = slice_nii_tensor(filtered_volume_tensor)
            segment_slices = slice_nii_tensor(segment_tensor)

            # Store the start index of slices for this volume
            start_idx = len(self.all_volume_slices)

            # Add slices to the global list
            self.all_volume_slices.extend(volume_slices)
            self.all_segment_slices.extend(segment_slices)

            # Store metadata for this volume
            end_idx = len(self.all_volume_slices)
            self.volume_metadata.append({
                "volume_idx": volume_idx,
                "slice_indices": list(range(start_idx, end_idx)),  # Indices of slices in global lists
                "volume_shape": filtered_volume_tensor.shape  # Original shape of the 3D volume
            })

        # Stack slices and add an additional channel dimension
        self.all_volume_slices = torch.stack(self.all_volume_slices).float()  # Ensure stacked slices are float
        self.all_segment_slices = torch.stack(self.all_segment_slices).float()  # Ensure stacked slices are float

        # Add a channel dimension for compatibility with models
        self.all_volume_slices = torch.unsqueeze(self.all_volume_slices, 1)
        self.all_segment_slices = torch.unsqueeze(self.all_segment_slices, 1)

    def __len__(self):
        return len(self.all_volume_slices)

    def __getitem__(self, idx):
        # Retrieve the specific slice and its metadata
        volume_slice = self.all_volume_slices[idx]
        segment_slice = self.all_segment_slices[idx]
        metadata = None
        for meta in self.volume_metadata:
            if idx in meta["slice_indices"]:
                metadata = meta
                break
        return volume_slice, segment_slice, metadata

"""### **Check Available Device**

- **Purpose**: Automatically selects the device (TPU, GPU, or CPU) for model training.
- **Steps**:
  1. Tries to use TPU if available.
  2. Falls back to GPU if TPU is unavailable.
  3. Uses CPU if neither TPU nor GPU is available.

This ensures that the model runs on the most powerful hardware available.

"""

# Check if TPU is available
try:
    import torch_xla.core.xla_model as xm
    device = xm.xla_device()  # Use TPU if available
    print(f"Using device: {device} (TPU)")
except:
    # Check if GPU is available
    if torch.cuda.is_available():
        device = "cuda"  # Use GPU
        print(f"Using device: {device} (GPU)")
    else:
        device = "cpu"  # Fall back to CPU
        print(f"Using device: {device} (CPU)")

"""### **Dataset Setup**

- **Purpose**: Prepare paths for volume and segmentation files and create a custom dataset.
- **Steps**:
  1. **Volume Paths**: List the `.nii` volume file paths.
  2. **Segmentation Paths**: List the `.nii` segmentation file paths.
  3. **Create Dataset**: Instantiate the `LiverTumorDataset` with paths and device (TPU, GPU, or CPU).

This prepares the dataset for training the model with the correct file paths.

"""

volume_paths = [
    f"/content/drive/MyDrive/Liver-Tumor-Segmentation-nnFormer/LITS17/volumes/volume-{i}.nii"
    for i in range(4,5)
]  # Add volume file paths for 0 to 9

segmentation_paths = [
    f"/content/drive/MyDrive/Liver-Tumor-Segmentation-nnFormer/LITS17/segmentations/segmentation-{i}.nii"
    for i in range(4,5)
]  # Add segmentation file paths for 0 to 9

# Create dataset
dataset = LiverTumorDataset(volume_paths, segmentation_paths, device=device)

"""### **Print Shape of Volume Slices**
- **Purpose**: Check the dimensions of all volume slices in the dataset.
- **Steps**:  
  - Use `torch.stack()` to combine all volume slices into a single tensor.
  - Print the shape of the tensor to ensure correct preprocessing.

This helps confirm that the slices have been correctly processed and formatted.

"""

all_volume_slices = dataset.all_volume_slices
all_volume_slices.shape

"""### **Display and Inspect Slices**

- **Purpose**: Display a specific slice from the dataset and check the total number of slices.
- **Steps**:
  1. **Display Slice**: Use `display_slice_from_dataset` to visualize a slice from the dataset at a specified index (`slice_index`).
  2. **Print Total Slices**: Print the total number of slices in the dataset to ensure everything is loaded correctly.

This is useful for verifying slice extraction and inspecting the dataset's size.

"""

# Display a specific slice
slice_index = 500  # Adjust as needed
display_slice(dataset, slice_index)

# Print total number of slices
print(f"Total number of slices in the dataset: {len(dataset)}")

"""#**Encoder**

"""

# prompt: create embedding layer input 512x512 which has no spatial loss and 3x3 filter 3 cov operations each followed by gelu and normalization and then 4th conv gelu and no normalization then create 64x64 each patches at last separate the conv and patches etxraction classes x shape

import torch
import torch.nn as nn
import torch.nn.functional as F

class EmbeddingLayer(nn.Module):
    def __init__(self, in_channels):
        super(EmbeddingLayer, self).__init__()
        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)
        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, padding=1)

    def forward(self, x):
        x = F.gelu(self.conv1(x))
        x = F.normalize(x, dim=1)
        x = F.gelu(self.conv2(x))
        x = F.normalize(x, dim=1)
        x = F.gelu(self.conv3(x))
        x = F.normalize(x, dim=1)
        x = F.gelu(self.conv4(x))
        return x

class PatchExtraction(nn.Module):
    def __init__(self, patch_size):
        super(PatchExtraction, self).__init__()
        self.patch_size = patch_size

    def forward(self, x):
        B, C, H, W = x.shape
        patches = x.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size)
        patches = patches.contiguous().view(B, C, -1, self.patch_size, self.patch_size).permute(0, 2, 1, 3, 4)
        return patches

# Example usage
if __name__ == "__main__":
    # Example input (replace with your actual input data)
    x = dataset.all_volume_slices[490:491]  # Batch size 1, 1 channel, 512x512 size

    # Create embedding layer and patch extraction instances
    embedding_layer = EmbeddingLayer(in_channels=1)
    patch_extraction = PatchExtraction(patch_size=64)

    # Forward pass
    embeddings = embedding_layer(x)
    patches = patch_extraction(embeddings)

    # Print shapes
    print("Embedding Layer Output Shape:", embeddings.shape)
    print("Patch Extraction Output Shape:", patches.shape)

# prompt: pactch size 64x64 implment an LocalWindowBased Self Attention Machanism whici will take input as  write all code at onceEmbedding Layer Output Shape: torch.Size([1, 512, 512, 512]) apply attention and then reshape it into this original shape

import torch
import torch.nn as nn
import torch.nn.functional as F

class LocalWindowAttention(nn.Module):
    def __init__(self, patch_size, embed_dim):
        super(LocalWindowAttention, self).__init__()
        self.patch_size = patch_size
        self.embed_dim = embed_dim

        self.query = nn.Linear(embed_dim, embed_dim)
        self.key = nn.Linear(embed_dim, embed_dim)
        self.value = nn.Linear(embed_dim, embed_dim)

    def forward(self, x):
        B, num_patches, C, H, W = x.shape  # Corrected shape

        # Reshape for linear layers
        x = x.view(B * num_patches, C, H * W)
        q = self.query(x)
        k = self.key(x)
        v = self.value(x)

        # Calculate attention scores
        attention_scores = torch.matmul(q.transpose(1, 2), k) / (self.embed_dim**0.5)
        attention_weights = F.softmax(attention_scores, dim=-1)

        # Apply attention
        attended_values = torch.matmul(attention_weights, v.transpose(1, 2))
        attended_values = attended_values.transpose(1, 2)

        # Reshape back to original patch shape
        attended_values = attended_values.view(B, num_patches, C, H, W)

        return attended_values

# Example usage (assuming you have the previous code)
if __name__ == "__main__":
    # ... (Your previous code for embedding and patch extraction)

    # Create LocalWindowAttention instance
    attention_layer = LocalWindowAttention(patch_size=64, embed_dim=512)

    # Apply attention to the patches
    attended_patches = attention_layer(patches)
    print("Attended Patches Shape:", attended_patches.shape)

    # Reshape back to original shape
    B, num_patches, C, H, W = attended_patches.shape
    reshaped_output = attended_patches.permute(0, 2, 1, 3, 4).contiguous().view(B, C, 512, 512)

    print("Reshaped Output Shape:", reshaped_output.shape)